---
title: "Business Analytics Capstone Final Project"
author: "Group9"
format: docx
editor: visual
---

**Sentimental Analysis comparing Performance Analysis of Predicting Sentiments using advance approaches of Machine Learning (Impacting People's Daily life with Products and Services Reviews)**

Guided By: Ahmet Ozkul

Group Members:

1\) Mohan Kamal Hassan

2\) Vinay Mathukumalli

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

12/08/2023

University Of New Haven

## Abstract

The Functional process of sentimental classification deals with the several process of mining the opinion retrieval, undergoing through the retrieval of the classification of the sentiments for the given product reviews where several feasibility of understanding and finding the following consequences of obtaining the classes of the particular reviews which are generated by the customer product reviews, while purchasing from the online Ecommerce website, thus demonstrating the means of the product reviews having its classes to be defined. Thus, our way of expressing the opinion for the particular product review makes a significant transformation towards both Industry and Consumer end. The examining of the classification of the product reviews given by each individual regarding to their product purchase. Dataset examined in this process are online product reviews collected from Amazon.com. In this proposed work, Analysis and comparison of the effectiveness of Hybrid Naive Bayes classifier for classification in conjunction with classifier methods present in each of the Machine Learning technique is implemented for Sentiment classification.

## Introduction

The Introduction to the Smart Phones and internet have made online shopping very easy. India's internet user base 354 million, registers 17% growth in first 6 months of 2015 IAMAI (Internet and Mobile Association of India.) report. The base had grown to 302 million by the end of 2014 after clocking its fastest rise of 32% in a year, as per IAMAI, which includes members such as Google, Microsoft, Facebook, and eBay, IBM, Amazon, Ola and LinkedIn.

Meanwhile the quality and delivery of products is uneven, thus users' comments become the important information to judge the product's quality and delivery time. At the same time, the product manufacturers can obtain the current main viewpoints from the users in order to improve the products.

A feature is defined to show both components and attributes and it is the subject of a review. In fact, people obey the grammatical rules to organize sentences while writing articles. But under informal circumstance, people usually neglect it and there are so many spelling mistakes. This phenomenon is especially prominent when people make comments after online shopping. The sentences have some different features comparing with the formal ones. 1) Products have a set of definite attributes and related opinion phrases. Thus we can use a small fixed set of keywords to recognize frequent feature and opinion words. 2) The opinionated sentences contain opinion operators which can be used to find positions of opinion expressions. 3) Many comment sentences are of free style, sometimes there are no opinion words in the comment sentences. If a feature is showed in the form of a noun or a noun phrase, then it is defined as the explicit feature. Meanwhile, the sentence which contains the explicit feature is recognized as the explicit sentence. According to the variety of the expression, we can divide the customer reviews into explicit sentence reviews and the ones without the explicit opinion feature are called implicit sentence reviews. The phenomenon that there is no explicit opinion feature in the sentence is very common in many comments. In our database which we extracted from the e-commerce website, the sentences without explicit opinion target make up to 30% approximately. For example: It is very cheap.We can deduce from the word "cheap" that the user may indicate the product's price. But the word "price" has not been directly mentioned but implied by the use of the word "cheap" which we can call feature indicator.

## Methodology

Sentiment analysis of natural language texts is a large and growing field. Sentiment analysis or Opinion Mining is the computational treatment of opinions and subjectivity of text. Sentiment analysis is an Information Extraction task that intends to acquire writer's feelings expressed in positive or negative comments, after analyzing his documents. The term 'Presence' is more important to sentiment analysis then term 'Frequency' which was earlier used for traditional information retrieval. It has also been reported that unigrams surpass bigrams for classifying movie reviews by sentiment polarity. Hatzivassiloglou and McKeown theorize that adjectives separated by "and" have the same polarity, while those separated by "but" have opposite polarity. Sentiment classification is a recent sub discipline of text classification which is concerned with opinion expressed by reviews. Opinion mining meansto determine whether a term that carries opinionated content has a positive or a negative in the implication.

This project is structured to analyze the sentiment of product reviews in two distinct ways: traditional keyword-based scoring and advanced machine learning classifiers. This approach aims to provide a comprehensive understanding of sentiments expressed in text data.

1.  Data Collection and Preprocessing: Initially, a dataset containing product reviews is collected. This dataset is then subjected to a series of preprocessing steps to convert the raw text into a format suitable for analysis. These steps include tokenization (breaking down text into individual words or phrases), removal of stopwords (common words that do not contribute much meaning, like 'the', 'is', etc.), and stemming (reducing words to their base form). The goal of preprocessing is to clean and simplify the text, making it easier to analyze.

2.  Method 1 - Keyword-Based Sentiment Analysis: The first method involves using a predefined list of words where each word is assigned a sentiment score (positive, negative, or neutral). This list is known as a sentiment lexicon, and in this project, the AFINN lexicon is used. Each review is scanned, and the words are matched with those in the lexicon. The sentiment scores of these words are then summed up to get an overall sentiment score for each review. Higher scores indicate positive sentiment, and lower scores indicate negative sentiment. This method is straightforward and relies heavily on the predefined sentiment scores of the words.

3.  Method 2 - Machine Learning-Based Sentiment Analysis: In contrast to the first method, the second approach involves training machine learning models to understand and predict sentiments. This process begins by converting the text data into a numerical format, typically using a technique that creates a matrix where each row represents a review and each column represents a word from the dataset. The cells in this matrix contain the frequency of each word in each review.

Several machine learning models are then employed, including:

-   Logistic Regression: Used for binary classification (positive or negative sentiment).

-   Naive Bayes Classifier: A probabilistic classifier that uses the Bayes theorem and is particularly good for text data.

-   Support Vector Machine (SVM): A classifier that finds the best boundary to separate different classes of data.

-   Random Forest: An ensemble method using multiple decision trees to improve prediction accuracy.

-   XGBoost: An efficient and scalable implementation of gradient boosting.

Each of these models is trained using a part of the dataset (training data) and then used to make predictions on another part (testing data). The predictions are compared against the actual sentiments to evaluate the performance of the models.

4.  Evaluation: The performance of both methods is evaluated using various metrics. For the machine learning models, confusion matrices and Receiver Operating Characteristic (ROC) curves are used. A confusion matrix shows the number of correct and incorrect predictions for each class, while ROC curves and the Area Under the Curve (AUC) provide insight into the trade-offs between true positive and false positive rates across different thresholds.

## Finding and Discussion

A. Text Pre-processing Text pre-processing techniques are divided into two subcategories. 1. Tokenization: Textual review data comprises block of characters called tokens. The review comments are separated as tokens and used for further processing. 2. Removal of Stop Words: A stop-list is the name commonly given to a set or list of stop words. It is typically Language specific, although it may contain words. Some of the more frequently used stop words for English include "a", "of", "the", "I", "it", "you", and "and" these are generally regarded as 'functional words' which do not carry meaning. When assessing the contents of natural language, the meaning can be conveyed more clearly by ignoring the functional words. Hence it is practical to remove those words which appear too often that support no information for the task. If the stop word removal is applied, all the stop words in the particular text file will not be loaded. If the stop word removal is not applied, the stop word removal algorithm will be disabled when the dataset is loaded. B. Text Transformation The score of each sentence in the source document is calculated by sum of weight of each term in the corresponding sentences. C. Feature Selection Many statistical feature selection methods for document level classification can also be used for sentiment analysis. The simplest statistical approach for feature selection is to use the most frequently occurring words in the corpus as polarity indicators. The majority of the approaches for sentiment analysis involve a two-step process: Identify the parts of the document to contribute the positive or negative sentiments.Join these parts of the document in ways that increase the odds of the document falling into one of these two polar categories.

MACHINE LEARNING METHODS AND IMPLEMENTATION I Phase 1. Fetch the comment. 2. Convert the unstructured comment data to structured document. 3. Tokenize the sentences into keywords. 4. Eliminate Stop words and tag the tokens using POS tagger. 5. If term is not in the dictionary check for the correct word. 6. Apply Nave Bayes classifier. 7. Applying Machine Learning Methods as parameter 8. Compute sentiments 9. Return sentiment and sentiment score of review II Phase 1. The first step is to read data. 2. Then we can build the document-term matrix. 3. We train the naive Bayes model, by using package e1071. 4. We train the model with multiple machine learning algorithms. 5. Identifying the Accuracy with several machine learning techniques. 6. Determining the effect of measuring parameters with machine learning techniques. 7. Summarizing of the result. 8. Analysis of Machine learning techniques to determine which method is accurate for sentiment analysis and classification of reviews. In this step, we classified the subjective sentences identified into positive and negative sentences. Calculating the polarity of each sentence is very important to determine the overall sentiment. The input for this process comprised all the identified subjective sentences

## Code Implementation

### Libraries

```{r}

library(tm) # for text mining operations
library(slam) # for dealing with sparse matrices
library(dplyr) # data frame handling
library(textdata) # for sentimental data
library(tidytext) # for tidying text
library(wordcloud) # for wordcloud generation
library(tidyverse) # for tidying
library(SnowballC) # for stemming tokens
library(RColorBrewer) # For ROC colors
library(yardstick) # for conf_mat
library(ramify) # for argmax in random forest
library(pROC) # for roc
library(e1071) # for logistic classifier
library(ggplot2) # plotting
library(ranger) # random forest
library(gridExtra) # for grid arrangement of ggplots
```

The above cell imports the libraries needed.

-   **`library(tm)`**: Loads the Text Mining (**`tm`**) package in R, essential for processing and analyzing the textual data in product reviews.

-   **`library(dplyr)`**: Activates the **`dplyr`** package, a powerful tool for data manipulation and transformations.

-   **`library(tidyverse)`**: Introduces the **`tidyverse`** suite of data science packages, enhancing capabilities for data manipulation, analysis, and visualizations.

-   **`library(tidytext)`**: Introduces functions for text mining and analysis within the tidy data framework, facilitating easy handling of textual data.

-   **`library(textdata)`**: Provides access to various text datasets and lexicons, useful for tasks like sentiment analysis and text enrichment.

-   **`library(SnowballC)`**: Enables text stemming capabilities, reducing words to their base or root form, a common preprocessing step in text analysis.

### Data Preparation

```{r}
# Importing data from local path
data_path <- "samples10kdataset.csv"
df <- read_csv(data_path, show_col_types = F)
head(df)
```

The above cell imports the data from a local path and displays the first 5 rows.

### Method 1 - Keyword Based Prediction

```{r}
tokens <- df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word))
```

In this above cell, the dataframe **`df`** undergoes a text preprocessing pipeline using functions primarily from the **`tidytext`** package. It begins with **`unnest_tokens`**, which breaks down the **`text`** field into individual words (tokens), thereby transforming the text data into a tidy format with one word per row. This tokenization process is essential for many text analysis tasks as it simplifies the handling of text data. Following tokenization, the pipeline removes common stopwords using **`anti_join(stop_words)`**, filtering out words that are typically irrelevant in text analysis, like 'the', 'is', and 'and'. Finally, **`mutate(word = wordStem(word))`** applies stemming to each word, reducing them to their root form using the **`SnowballC`** package. This stemming process helps in consolidating different forms of a word (like 'running', 'ran', 'runs') into a single, base form ('run'), facilitating more efficient and effective text analysis.

```{r}
afinn <- get_sentiments("afinn")

sentiment <- tokens %>%
  inner_join(afinn, by = "word") %>%
  group_by(rating) %>%
  summarize(sentiment = sum(value))
```

In the above cell, sentiment analysis is conducted on the previously processed **`tokens`** using the AFINN lexicon, a popular word list for sentiment analysis:

-   The first line retrieves the AFINN sentiment lexicon using the **`get_sentiments`** function from the **`textdata`** package. The AFINN lexicon assigns each word a sentiment score, where positive words have positive scores, negative words have negative scores, and neutral words have scores around zero.

-   The second part of the code performs sentiment scoring of the **`tokens`**. It begins with an **`inner_join`** operation, merging the tokens with the AFINN lexicon based on matching words. This merge essentially assigns a sentiment score to each word in the tokens. Following this, the **`group_by(rating)`** function categorizes the data by the **`rating`** variable, allowing for sentiment scores to be aggregated for each rating category. Finally, **`summarize(sentiment = sum(value))`** calculates the total sentiment score for each rating group by summing up the individual word scores.

```{r}

predict_sentiment <- function(new_review) {
  tokens_new <- tibble(text = new_review) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words) %>%
    mutate(word = wordStem(word))

  sentiment_score <- tokens_new %>%
    inner_join(afinn, by = "word") %>%
    summarize(sentiment = sum(value)) %>%
    pull(sentiment)
  
  return(sentiment_score)
}
```

The **`predict_sentiment`** function defined in the above cell is designed to analyze the sentiment of a new review. When this function is called with a text input (**`new_review`**), it first converts the input text into a tidy tibble format and then applies a series of text preprocessing steps, including tokenization, stop word removal, and word stemming. These steps transform the text into a standardized form suitable for sentiment analysis. The function then calculates the sentiment score of the processed text by joining it with the AFINN sentiment lexicon (**`afinn`**), which associates each word with a sentiment value. It sums these values to derive an overall sentiment score for the input review. The final score, which is extracted using **`pull(sentiment)`**, quantifies the emotional tone of the review, with higher scores indicating more positive sentiment and lower scores indicating more negative sentiment.

```{r message=F}

good_review = "I'm glad I purchased this product, this is so amazing."
good_score <- predict_sentiment(good_review)
cat("[GOOD]", good_review, ":", good_score)
```

```{r message=F}

neutral_review = "The product is okay and average, It is not great"
neutral_score <- predict_sentiment(neutral_review)
cat("[NEUTRAL]", neutral_review, ":", neutral_score)
```

```{r message=F}

bad_review = "This is really bad, It is a waste of money. I'm disappointed"
bad_score <- predict_sentiment(bad_review)
cat("[BAD]", bad_review, ":", bad_score)
```

```{r}
cat("[GOOD]", good_review, ":", good_score, "\n")
cat("[NEUTRAL]", neutral_review, ":", neutral_score, "\n")
cat("[BAD]", bad_review, ":", bad_score, "\n")
```

```{r}

rating_hist <- sentiment %>%
  ggplot(aes(x = rating, y = sentiment)) +
  geom_col() +
  labs(title = "Sentiment Analysis by Rating", y = "Sentiment Score") +
  theme_minimal()

rating_hist
```

The bar chart visualizes the frequencies of sentiment scores associated with product ratings on a scale from 1 to 5. The sentiment scores are aggregated from individual words or phrases within the reviews that have been scored using a sentiment analysis method, AFINN system mentioned earlier.

### Method 2 - Machine Learning Based Classifiers

```{r}

# Converting the text field to a vector source and then converting it to vector corpus
corpus <- VCorpus(VectorSource(df$text))
corpus
```

The above cell employs the functionality of the `tm` package in R to handle text data from product reviews. By utilizing `VectorSource` and `VCorpus`, it efficiently converts the text column from the dataframe into a corpus. A corpus, in text mining terms, is a collection of text documents stored in a structured manner. This conversion is a crucial step, as it lays the groundwork for the subsequent text mining procedures. It facilitates various operations on the text data, such as cleaning, normalization, and transformation, which are essential for converting the unstructured text into a numerical form that can be used for predictive modeling or other analytical tasks.

```{r}

# A function to clean corpus.
# Performs:
# 1. Stripping white spaces
# 2. Removes punctuation symbols
# 3. Coverts to lower case text
# 4. Removes numbers
# 5. Removes stop words

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("en"))
  return(corpus)
}
```

This above cell defines a function `clean_corpus` designed to perform a series of standard text cleaning operations on a corpus. The function takes a corpus as input, which is a collection of text documents, and applies various `tm_map` functions from the `tm` package in R to preprocess the text.

The cleaning steps include:

1.  `stripWhitespace`: Removes extra white spaces from the text, ensuring that unnecessary spaces don't affect the analysis.
2.  `removePunctuation`: Eliminates punctuation marks. Punctuation typically doesn't carry semantic meaning and can be removed to simplify the text.
3.  `content_transformer(tolower)`: Converts all text to lowercase, which helps in standardizing the text and ensuring that the same words in different cases are treated identically.
4.  `removeNumbers`: Discards numerical digits, which are often not needed in text analysis unless specific numeric information is crucial to the context.
5.  `removeWords`: Eliminates common stopwords using the predefined "en" (English) stopwords list from the `tm` package. Stopwords are common words like 'the', 'is', and 'in', which usually don't contribute to the overall meaning of the text and can be removed.

After applying these transformations, the function returns the cleaned corpus. This preprocessing is an essential step in text analysis, as it reduces noise and complexity in the text data, making it more amenable to build document-term matrix.

```{r}

# Clean the corpus by mapping raw corpus to clean corpus function.
clean_corpus <- clean_corpus(corpus)
clean_corpus
```

```{r}

# Convert cleaned corpus to document term matrix
dtm <- DocumentTermMatrix(clean_corpus)
dtm
```

The **`DocumentTermMatrix`** function from the **`tm`** package is used to transform the cleaned corpus into a DTM. In the context of text mining, a DTM is a matrix where rows represent documents (in this case, individual product reviews) and columns represent terms (usually words). Each entry in the matrix denotes the frequency of a term in a document.

```{r}

# Convert the DTM object to a matrix
dtm_matrix <- as.matrix(dtm)
dim(dtm_matrix)
```

The above cell converts the DTM object to a matrix and prints the dimensions. It has too many columns (terms). For this reason, we have to perform dimensionality reduction. There are many ways to perform dimensionality reduction, but this project employs removing the sparse terms. This operation drops the columns where the term is rarely used. The threshold for dropping is set to 0.98, indicating to drop a column if 98% of the rows are zeros.

```{r}

# Drop the sparse terms
dtm_matrix_non_sparse <- removeSparseTerms(dtm, 0.98)
dtm_df <- as.data.frame(as.matrix(dtm_matrix_non_sparse))
dim(dtm_df)
```

The above action of removing sparse terms using `removeSparseTerms` with a threshold of 0.98 has drastically dropped number of columns.

```{r}

stm <- as.simple_triplet_matrix(as.matrix(dtm_matrix_non_sparse))
tf_vector <- sort(col_sums(stm), decreasing=T)
wordcloud(names(tf_vector), tf_vector, scale=c(3, 0.5), colors=brewer.pal(8, "Dark2"), random.order = F)
```

The above cell creates a word cloud from the frequency of the words.

```{r}

dtm_df$rating <- as.factor(df$rating)
```

This command is adding a new column named **`rating`** to the dataframe **`dtm_df`**. The content of this new column is sourced from the **`rating`** column of main dataframe **`df`**. The key operation here is the conversion of the **`rating`** data into a factor using the **`as.factor()`** function.

To perform supervised learning, the conversion of the **`rating`** column into a factor is significant. Ratings, which originally are in a numeric format, are treated as categorical data when converted to factors. This is particularly important in R, where modeling functions distinguish between numeric and categorical variables, and treat them differently in the analysis.

### Data Modelling

```{r}

set.seed(123)

train_pct = 0.8
train_indices <- sample(1:nrow(dtm_df), train_pct * nrow(dtm_df))

train_data <- dtm_df[train_indices, ]
test_data <- dtm_df[-train_indices, ]
```

The above cell implements a reproducible split of the dataset **`dtm_df`** into training and test sets, with 80% of the data allocated for training and the remaining 20% for testing, to facilitate the training and subsequent evaluation of predictive models. The use of **`set.seed`** ensures that this random partitioning is consistent across multiple runs.

```{r}

table(train_data$rating)
```

```{r}

table(test_data$rating)
```

The above tables confirms that the split is nearly balanced among different classes.

```{r}

# Separating features and labels
train_data_features <- train_data[, -which(names(df) == "rating")]
train_data_labels <- train_data$rating

test_data_features <- test_data[, -which(names(df) == "rating")]
test_data_labels <- test_data$rating
```

The above cell separates the features and labels into individual variables.

```{r}

sum(is.na(train_data_labels))
```

The 0 in the output of above cell confirms that there is no missing data in training set.

```{r}

train_data_binary <- train_data
train_data_binary$rating <- ifelse(as.numeric(train_data_labels) > 3, 1, 0)

test_data_binary <- test_data
test_data_binary$rating <- ifelse(as.numeric(test_data_labels) > 3, 1, 0)
```

```{r}

total_train_predictions <- data.frame(true=train_data_labels)
total_test_predictions <- data.frame(true=test_data_labels)
```

#### Logistic Classifier (Binary)

Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although extensions to the model exist to handle multi-class classification problems. It estimates the probability that a given input belongs to a certain category, making it a popular choice for binary classification tasks. The output of a logistic regression model is a probability that the given input point belongs to a certain class, which can be turned into a binary outcome using a threshold classifier. Logistic regression is robust to small sample sizes, can easily incorporate continuous and categorical data, and provides interpretable results, as the coefficients represent the log odds of the outcome and can be used to infer the importance of features. Although it is possible to use external library `nnet` to implement `multinom` that fits multi-class, this project doesn't implement it because of the number of columns for this specific data. It would throw `too many weights` error. For this reason, this project simply employs a binary classification task assuming rating above 3 is positive(1) and below or equal to 3 is negative(0).

```{r}

log_model <- glm(rating ~ ., data=train_data_binary, family = binomial)
```

```{r}

# Training confusion matrix
log_predictions_probs_train <- predict(log_model, newdata=train_data_binary, type="response")
log_predictions_train <- ifelse(log_predictions_probs_train > 0.5, 1, 0)
log_cm_train <- conf_mat(data.frame(true=as.factor(train_data_binary$rating), log_pred_train=as.factor(log_predictions_train)), true, log_pred_train)
log_cm_train
```

```{r}

summary(log_cm_train)
```

```{r}

# Testing confusion matrix
log_predictions_probs <- predict(log_model, newdata=test_data_binary, type="response")
log_predictions <- ifelse(log_predictions_probs > 0.5, 1, 0)
log_cm <- conf_mat(data.frame(true=as.factor(test_data_binary$rating), log_pred=as.factor(log_predictions)), true, log_pred)
log_cm
```

```{r}

summary(log_cm)
```

```{r}

# Confusion matrices plot
log_cm_train_plot <- autoplot(log_cm_train, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Logistic Regression - Train")

log_cm_test_plot <- autoplot(log_cm, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Logistic Regression - Test")

grid.arrange(log_cm_train_plot, log_cm_test_plot, ncol=2)
```

```{r}

plot(roc(response = test_data_binary$rating, predictor = log_predictions_probs), main = "ROC - Logistic Regression Classifier", print.auc = T)
```

The above cells describes the process of adapting a dataset for binary classification with logistic regression and evaluating the model. The training and test sets, **`train_data`** and **`test_data`**, are first modified to create binary outcome variables where ratings greater than 3 are labeled as 1 (positive class) and others as 0 (negative class), effectively converting a multi-class classification problem into a binary one. A logistic regression model is then trained on this binary training dataset using the **`glm`** function with a binomial family, which is appropriate for binary outcomes. Predicted probabilities are generated for the test dataset, and these probabilities are converted into binary predictions using a 0.5 threshold. Finally, a confusion matrix is created to summarize the performance of the model, comparing the predicted binary outcomes against the actual binary ratings from the test set. This matrix is a critical tool for assessing the model's accuracy, providing insight into its true positive and false positive rates. The area under the curve (AUC) for ROC is 0.759, indicating a good predictive performance of the classifier.

#### Naive Bayes Classifier

The Naive Bayes classifier is a probabilistic machine learning model based on Bayes' Theorem, which predicts the probability of a label given a set of features. It is called 'naive' because it assumes independence between the features, a strong assumption that simplifies the computation but is often violated in practice. Despite this, Naive Bayes can perform surprisingly well in many real-world scenarios, particularly in text classification and spam filtering, due to its simplicity, efficiency, and ability to handle high-dimensional data.

```{r}

nb_model <- naiveBayes(rating ~ ., data=train_data)
```

```{r}

# Training confusion matrix
total_train_predictions$nb <- predict(nb_model, newdata=train_data)
nb_cm_train <- conf_mat(total_train_predictions, true, nb)
nb_cm_train
```

```{r}

summary(nb_cm_train)
```

```{r}

# Testing confusion matrix
total_test_predictions$nb <- predict(nb_model, newdata=test_data)
nb_cm_test <- conf_mat(total_test_predictions, true, nb)
nb_cm_test
```

```{r}

summary(nb_cm_test)
```

```{r}

# Confusion matrices plot
nb_cm_train_plot <- autoplot(nb_cm_train, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Naive Bayes Classifier - Train")

nb_cm_test_plot <- autoplot(nb_cm_test, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Naive Bayes Classifier - Test")

grid.arrange(nb_cm_train_plot, nb_cm_test_plot, ncol=2)
```

In the above cells, a Naive Bayes classifier is built and evaluated using the **`e1071`** package in R. After loading the necessary library, the **`naiveBayes`** function is used to train the model using the **`train_data`** dataset, with **`rating`** as the response variable. Once the model is trained, it is then used to make predictions on the **`test_data`**. The accuracy of these predictions is assessed by constructing a confusion matrix, comparing the predicted labels against the actual labels from **`test_data`**. The resulting confusion matrix, stored in **`nb_confusionMatrix`**, provides a tabular representation of the model's performance, detailing the correct and incorrect predictions across the different rating classes.

```{r}

nb_predictions_probs <- predict(nb_model, newdata=test_data, type="raw")
class_levels <- levels(test_data_labels)
nb_roc_scores <- lapply(class_levels, function(class) {
  # Creating a binary response for the current class (one-vs-rest)
  binary_response <- ifelse(test_data_labels == class, 1, 0)
  
  # Predicted probabilities for the current class
  class_probs <- nb_predictions_probs[, class]
  
  # Compute ROC curve
  roc(response = binary_response, predictor = class_probs)
})
```

```{r}

plot(nb_roc_scores[[1]], col = 1, main = "Multiclass ROC - Naive Bayes Classifier", print.auc = T)
sapply(2:length(nb_roc_scores), function(i) {
    lines(nb_roc_scores[[i]], col = i, print.auc = TRUE)
})
legend("bottomright", legend = class_levels, col = 1:length(class_levels), lwd = 2)
```

The above cells evaluate the performance of the Naive Bayes classifier by generating ROC curves for multiclass classification. The **`pROC`** library is utilized to calculate the ROC statistics. Predicted probabilities are obtained for each class in **`test_data`** using the **`predict`** function with **`type="raw"`**, enabling the computation of ROC curves. The **`levels`** function is used to capture the unique class levels, and **`lapply`** is employed to iterate over each class level to compute a one-vs-rest ROC curve, assessing the classifier's ability to distinguish each class from the rest. These ROC curves are then plotted, with the AUC (Area Under the Curve) displayed for a visual comparison of classifier performance across classes.

In the above plot, each colored line represents the ROC curve for one of the five classes, indicating the trade-off between sensitivity (true positive rate) and 1-specificity (false positive rate) for each class. The AUC (Area Under the Curve) for the first class is specifically annotated as 0.735. Generally, an AUC closer to 1 implies a better performing model, while an AUC closer to 0.5 suggests no discriminative power. The presence of multiple curves suggests a one-vs-rest approach to multi-class classification, where each class is separated and evaluated against all others.

#### SVM

Support Vector Machine (SVM) is a powerful and versatile supervised machine learning algorithm, used primarily for classification tasks, but it's also effective in regression. It works by finding the optimal hyperplane that best separates different classes in the feature space. In simpler terms, SVM attempts to draw a line (in two dimensions) or a manifold (in higher dimensions) that divides different class labels with the widest possible margin. One of the key strengths of SVM is its use of kernel functions, which allows it to handle non-linear data by mapping the original features into a higher-dimensional space where a linear separation is possible. This makes SVM highly effective in complex datasets where classes are not linearly separable. SVM is known for its robustness, especially in cases where the number of features is larger than the number of samples, and is widely used in various fields such as image classification, bioinformatics, and text categorization.

```{r}

svm_model <- svm(rating ~ ., data=train_data)
```

```{r}

# Training confusion matrix
total_train_predictions$svm <- predict(svm_model, newdata=train_data)
svm_cm_train <- conf_mat(total_train_predictions, true, svm)
svm_cm_train
```

```{r}

summary(svm_cm_train)
```

```{r}

# Testing confusion matrix
total_test_predictions$svm <- predict(svm_model, newdata=test_data)
svm_cm_test <- conf_mat(total_test_predictions, true, svm)
svm_cm_test
```

```{r}

summary(svm_cm_test)
```

```{r}

# Confusion matrices plot
svm_cm_train_plot <- autoplot(svm_cm_train, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("SVM Classifier - Train")

svm_cm_test_plot <- autoplot(svm_cm_test, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("SVM Classifier - Test")

grid.arrange(svm_cm_train_plot, svm_cm_test_plot, ncol=2)
```

The above cells creates, trains, and evaluates a Support Vector Machine (SVM) model for classification. Initially, the SVM model is trained using the `svm` function from `e0171` library to predict the 'rating' variable based on all other features in the 'train_data' dataset. The model's performance on the training data is then evaluated by generating a confusion matrix using the `conf_mat` function.

Then the trained SVM model is used to predict outcomes on 'test_data' dataset. Finally, the model's performance on this test dataset is evaluated by creating another confusion matrix ('svm_cm_test'), which helps in assessing the generalization capability of the model and understanding how well it performs on unseen data.

#### Random Forest Classifier (Ranger)

Random Forest is an ensemble machine learning algorithm that operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) of the individual trees. The **`ranger`** library in R is an implementation of the Random Forest algorithm, known for its performance and capability to handle very large datasets efficiently. It builds a large collection of uncorrelated decision trees and aggregates their predictions. This method is highly accurate and capable of modeling complex interactions and non-linear relationships between features and the target variable. Random Forests also have built-in methods for estimating feature importance, making them useful not only for prediction but also for understanding the influencing factors behind a model's decisions.

```{r}

rf_model <- ranger(x=train_data_features, y=train_data_labels, num.trees=100, min.node.size=100, probability = TRUE, mtry=1)
```

```{r}

summary(rf_model)
```

```{r}

rf_predictions_probs_train <- predict(rf_model, data=train_data_features, probabilty = TRUE)
rf_predictions_train <- argmax(rf_predictions_probs_train$predictions, rows = TRUE)
total_train_predictions$rf <- as.factor(rf_predictions_train)
rf_cm_train <- conf_mat(total_train_predictions, true, rf)
rf_cm_train
```

```{r}

summary(rf_cm_train)
```

```{r}

rf_predictions_probs_test <- predict(rf_model, data=test_data_features, probabilty = TRUE)
rf_predictions_test <- argmax(rf_predictions_probs_test$predictions, rows = TRUE)
total_test_predictions$rf <- as.factor(rf_predictions_test)
rf_cm_test <- conf_mat(total_test_predictions, true, rf)
rf_cm_test
```

```{r}

summary(rf_cm_test)
```

```{r}

# Confusion matrices plot
rf_cm_train_plot <- autoplot(rf_cm_train, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Random Forest Classifier - Train")

rf_cm_test_plot <- autoplot(rf_cm_test, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("Random Forest Classifier - Test")

grid.arrange(rf_cm_train_plot, rf_cm_test_plot, ncol=2)
```

```{r}

rf_roc_scores <- lapply(class_levels, function(class) {
  # Creating a binary response for the current class (one-vs-rest)
  binary_response <- ifelse(test_data_labels == class, 1, 0)
  
  # Predicted probabilities for the current class
  class_probs <- rf_predictions_probs_test$predictions[, class]
  
  # Compute ROC curve
  roc(response = binary_response, predictor = class_probs)
})
```

```{r}

plot(rf_roc_scores[[1]], col = 1, main = "Multiclass ROC - Random Forest Classifier", print.auc = T)
sapply(2:length(rf_roc_scores), function(i) {
    lines(rf_roc_scores[[i]], col = i, print.auc = TRUE)
})
legend("bottomright", legend = class_levels, col = 1:length(class_levels), lwd = 2)
```

The above cells summarize a Random Forest model using the **`ranger`** package in R, which is known for its speed and ability to handle large datasets.

A Random Forest model, **`rf_model`**, is trained with specific parameters: 100 trees (**`num.trees=100`**) and a minimum node size of 50 (**`min.node.size=50`**). The training features (**`train_data_features`**) and labels (**`train_data_labels`**) are provided as inputs to the model. After training, the **`summary`** function is called to output details about the model, providing insights into its structure and performance metrics.

The model is then used to predict outcomes on the training data to compute a confusion matrix (**`rf_confusionMatrix_train`**). This matrix compares the actual labels of the training data against the predictions made by the model. This step is crucial for understanding the model's performance on the data it was trained on, which includes checking for overfitting and ensuring that the model has learned the patterns effectively.

#### XGBoost Classifier

XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting algorithms, renowned for its efficiency, flexibility, and portability. It stands out in the field of machine learning for its ability to perform both classification and regression tasks. XGBoost works by building an ensemble of decision trees in a sequential manner, where each tree attempts to correct the errors of its predecessor, leading to a robust predictive model. It includes features like handling of missing data, regularization to avoid overfitting, and efficient use of system resources, making it highly scalable. This algorithm has gained immense popularity due to its effectiveness in various Kaggle competitions and its broad application in industries ranging from finance to healthcare for predictive modeling tasks.

```{r}

library(xgboost)

xgb.train = xgb.DMatrix(data=as.matrix(as.data.frame(sapply(train_data_features, as.numeric))), label=train_data_labels)
xgb.test = xgb.DMatrix(data=as.matrix(as.data.frame(sapply(test_data_features, as.numeric))), label=test_data_labels)

params = list(booster="gbtree", eta=0.0001, num_class=ncol(train_data_features), objective="multi:softprob")

xgb_model <- xgb.train(params=params, data=xgb.train, nrounds=3)
```

```{r}
xgb_predictions_probs_train <- predict(xgb_model, as.matrix(as.data.frame(sapply(train_data_features, as.numeric))), reshape=T)
xgb_predictions_train <- argmax(xgb_predictions_probs_train, rows = TRUE)
total_train_predictions$xgb <- as.factor(xgb_predictions_train - 1)
xgb_cm_train <- conf_mat(total_train_predictions, true, xgb)
xgb_cm_train
```

```{r}
summary(xgb_cm_train)
```

```{r}

xgb_predictions_probs_test <- predict(xgb_model, as.matrix(as.data.frame(sapply(test_data_features, as.numeric))), reshape=T)
xgb_predictions_test <- argmax(xgb_predictions_probs_test, rows = TRUE)
total_test_predictions$xgb <- as.factor(xgb_predictions_test - 1)
xgb_cm_test <- conf_mat(total_test_predictions, true, xgb)
xgb_cm_test
```

```{r}

summary(xgb_cm_test)
```

```{r}

# Confusion matrices plot
xgb_cm_train_plot <- autoplot(xgb_cm_train, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("XGBoost Classifier - Train")

xgb_cm_test_plot <- autoplot(xgb_cm_test, type="heatmap") +
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") +
  theme(legend.position = "right") + 
  ggtitle("XGBoost Classifier - Test")

grid.arrange(xgb_cm_train_plot, xgb_cm_test_plot, ncol=2)
```

The above cells demonstrates the process of training and evaluating a machine learning model using the **`xgboost`** library, specifically for a multi-class classification task. Training and testing datasets are prepared using the **`xgb.DMatrix`** function, converting feature data to a matrix format and attaching corresponding labels.

The model parameters are defined in a list named **`params`**. It specifies the use of gradient boosted trees (**`gbtree`**) as the learning algorithm, sets a learning rate (**`eta`**) of 0.001, the number of classes in the dataset (**`num_class`**), and the objective function (**`objective`**) to **`multi:softprob`** which is suitable for multi-class classification problems. The training of the model is initiated with the **`xgb.train`** function, where the defined parameters, training data, number of training rounds (**`nrounds`**).

After training, predictions are made for both the training and testing datasets. The **`predict`** function is used to generate probabilities of each class for each instance. The **`argmax`** function then identifies the class with the highest probability for each instance, resulting in the final class predictions. These predictions are converted into factors and stored in **`total_train_predictions`** and **`total_test_predictions`** for training and testing datasets, respectively.

Finally, confusion matrices for both the training and testing predictions are created using the **`conf_mat`** function, which provides a detailed summary of the model's performance in terms of its classification accuracy and misclassifications. These **`conf_mat`** matrices are then plotted using **`ggplot`**.

## Conclusions

In summary, this project aimed to identify the sentiment of Amazon reviews and used two main methods. The first was a basic technique using AFINN sentiment scores, which worked by matching words to scores. This method is simple but might not catch all the nuances in sentiment. The second method used more advanced machine learning models like Logistic Regression, Naive Bayes Classifier, SVM, Random Forest, and XGBoost. These models treated sentiments as ratings from 1 to 5. Out of these, Random Forest and XGBoost performed really well, with XGBoost being the standout. This shows that while basic methods are good for a quick sentiment check, more complex models like XGBoost can provide a deeper and more accurate analysis of how people feel based on their reviews.

## References

\[1\] S. ChandraKala1 and C. Sindhu2, "OPINION MINING ANDSENTIMENT CLASSIFICATION: A SURVEY,".Vol .3(1),Oct-2012,420-427.

\[2\] Kim S-M, Hovy E (2004) Determining the sentiment of opinions In: Proceedings of the 20th international conference on Computational Linguistics, page 1367.Association for Computational Linguistics, Strasbourg, PA, USA.

\[3\] Liu B (2010) Sentiment analysis and subjectivity In: Handbook of Natural Language Processing, Second Edition.. Taylor and Francis Group, Boca

\[4\] Liu B, Hu M, Cheng J (2005) Opinion observer: Analyzing and comparing opinions on the web In: Proceedings of the 14th International Conference on World Wide Web, WWW '05, 342--351.ACM, New York, NY, USA.

\[5\] Pang B, Lee L (2004) A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts In: Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL '04.Association for Computational Linguistics, Stroudsburg, PA, USA

```{r message=F}
#| echo: false
{r}

library(shiny)
ui <- fluidPage(
  titlePanel("Sentimental Analysis"),
  tabsetPanel(
    tabPanel("Ratings", plotOutput("ratings_plot"), div(textOutput("ratings_caption"), style = "text-align:center;")),
    tabPanel("Wordcloud", plotOutput("wordcloud", width="100%", height="500px"), div(textOutput("wc_caption"), style = "text-align:center;")),
    tabPanel("Logistic Regression", plotOutput("log_reg"), div(textOutput("log_reg_caption"), style = "text-align:center;")),
    tabPanel("Naive Bayes", plotOutput("nb"), div(textOutput("nb_caption"), style = "text-align:center;")),
    tabPanel("SVM", plotOutput("svm"), div(textOutput("svm_caption"), style = "text-align:center;")),
    tabPanel("Random Forest", plotOutput("rf"), div(textOutput("rf_caption"), style = "text-align:center;")),
    tabPanel("XGBoost", plotOutput("xgb"), div(textOutput("xgb_caption"), style = "text-align:center;")),
  )
)

server <- function(input, output){
  output$ratings_plot <- renderPlot({
    rating_hist
  })
  output$ratings_caption <- renderText({
    "Frequency chart of ratings from the dataset"
  })
  
  output$wordcloud <- renderPlot({
    wordcloud(names(tf_vector), tf_vector, scale=c(4, 0.5), colors=brewer.pal(8, "Dark2"), random.order = F)
  })
  output$wc_caption <- renderText({
    "Wordcloud of the words from the dataset"
  })
  
  output$log_reg <- renderPlot({
    arranged_grid <- grid.arrange(log_cm_train_plot, log_cm_test_plot, ncol=2)
  })
  output$log_reg_caption <- renderText({
    "Confusion matrices of the training(80% of data) and testing (20% of data) when trained using Logistic Regression Classifer"
  })
  
  output$nb <- renderPlot({
    grid.arrange(nb_cm_train_plot, nb_cm_test_plot, ncol=2)
  })
  output$nb_caption <- renderText({
    "Confusion matrices of the training(80% of data) and testing (20% of data) when trained using Naive Bayes Classifer"
  })
  
  output$svm <- renderPlot({
    grid.arrange(svm_cm_train_plot, svm_cm_test_plot, ncol=2)
  })
  output$svm_caption <- renderText({
    "Confusion matrices of the training(80% of data) and testing (20% of data) when trained using Support Vector Machines Classifer"
  })
  
  output$rf <- renderPlot({
    grid.arrange(rf_cm_train_plot, rf_cm_test_plot, ncol=2)
  })
  output$rf_caption <- renderText({
    "Confusion matrices of the training(80% of data) and testing (20% of data) when trained using Random Forest Classifer"
  })
  
  output$xgb <- renderPlot({
    grid.arrange(xgb_cm_train_plot, xgb_cm_test_plot, ncol=2)
  })
  output$xgb_caption <- renderText({
    "Confusion matrices of the training(80% of data) and testing (20% of data) when trained using XGBoost Classifer"
  })

}

shinyApp(ui = ui, server = server)
```
